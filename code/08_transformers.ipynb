{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://certificate.tpq.io/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workshop at Texas State University (October 2023)**\n",
    "\n",
    "**_Transformers & Attention_**\n",
    "\n",
    "Dr. Yves J. Hilpisch | The Python Quants GmbH | http://tpq.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Let's start with the basics:\n",
    "\n",
    "### Transformer\n",
    "\n",
    "Imagine you're reading a book, and while reading a sentence, you remember something related from a few pages back. This connection helps you better understand the current sentence. A transformer does something similar but for machines. It's a method that helps machines pay \"attention\" to different parts of data (like words in a sentence) to understand it better. It can look at all words at once and decide which ones are important in context.\n",
    "\n",
    "For example, in the sentence \"He dropped the glass because it was too hot,\" understanding \"hot\" helps us know why the glass was dropped.\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "GPT is like a super-smart student. Before giving it any specific task, it reads (or \"trains\" on) a huge amount of text from the internet. This is the \"pre-trained\" part. It learns language, facts, reasoning abilities, and even some mistakes from this data. Then, when you give it a specific question or task, it uses all that knowledge to generate (or \"write\") a coherent and contextually relevant response.\n",
    "\n",
    "### Other Key Topics:\n",
    "\n",
    "1. **Attention**: This is like the spotlight of our memory. When we think or read, we don't give equal importance to everything. Some parts get more focus because they're more relevant at the moment. In transformers, \"attention\" helps the model decide which parts of the data to focus on more.\n",
    "\n",
    "2. **Embeddings**: Think of this as translating words into a secret language that computers understand better. This language is in the form of numbers. So, \"cat\" might be translated to a list of numbers, which captures the essence or meaning of \"cat\" in this numeric form.\n",
    "\n",
    "3. **Fine-tuning**: Remember our super-smart student, GPT? After it learns from the internet, it can be given specific examples to make it even better at certain tasks. This process is like giving it extra classes on a particular subject.\n",
    "\n",
    "4. **Self-Attention**: This is a special feature of transformers. When trying to understand a word, the model looks at all other words in the sentence, not just neighboring words. It's like reading a whole paragraph to understand a single complex word in it.\n",
    "\n",
    "5. **Layers**: Transformers have multiple layers, just like how a complex thought might be built on simpler ideas. Each layer in a transformer captures different levels of understanding from the data.\n",
    "\n",
    "6. **Tokens**: These are chunks of text the model looks at. A token can be as short as one character or as long as one word.\n",
    "\n",
    "In essence, transformers, like GPT, are advanced tools that help machines understand and generate human-like text by focusing on the right parts of data, remembering what they've learned, and building complex understanding layer by layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "_**Just discoverd**:_\n",
    "\n",
    "This is an interesting article about embeddings with several examples and illustrations.\n",
    "\n",
    "[Embeddings: What they are and why they matter](https://simonwillison.net/2023/Oct/23/embeddings/)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are a fundamental concept in natural language processing (NLP) and deep learning. They provide a way to represent words (or other entities) as dense vectors of real numbers. The idea is to represent semantic meaning in a continuous vector space, where the distance and direction between vectors correspond to semantic differences between the words.\n",
    "\n",
    "### Importance of Embeddings:\n",
    "\n",
    "1. **Dimensionality Reduction**: Natural language has a vast vocabulary. One-hot encoding words would result in extremely high-dimensional vectors with a dimension equal to the vocabulary size. Embeddings reduce this to a manageable size (e.g., 50, 100, 300 dimensions).\n",
    "  \n",
    "2. **Semantic Relationships**: Good embeddings capture semantic relationships. For instance, the vector difference between \"king\" and \"man\" might be similar to the difference between \"queen\" and \"woman\".\n",
    "  \n",
    "3. **Transfer Learning**: Pre-trained embeddings can be used in new tasks, allowing models to benefit from knowledge captured in the embeddings without requiring extensive training data for the new task.\n",
    "\n",
    "### How are they derived?\n",
    "\n",
    "There are various methods to derive word embeddings:\n",
    "\n",
    "1. **Word2Vec**: Uses a shallow neural network to either predict a word given its context (Skip-Gram) or predict the context given a word (CBOW). The weights of this model then serve as the word embeddings.\n",
    "  \n",
    "2. **GloVe (Global Vectors for Word Representation)**: Constructs a co-occurrence matrix from a corpus and then factorizes this matrix to produce embeddings.\n",
    "  \n",
    "3. **FastText**: An extension of Word2Vec, it treats each word as composed of character n-grams. This allows it to generate embeddings for out-of-vocabulary words.\n",
    "  \n",
    "4. **Embeddings from Deep Learning Models**: Models like ELMo, BERT, and GPT generate contextual embeddings where the embedding for a word is based on its context in a sentence, and not just the word itself.\n",
    "\n",
    "### Python Example with Word2Vec:\n",
    "\n",
    "We'll use the `gensim` library to demonstrate Word2Vec. Let's create a simple model using some sentences.\n",
    "\n",
    "(Note: For this example, I'll provide code and expected outputs since we can't run the `gensim` library directly here.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['hello', 'world'],\n",
    "    ['hello', 'everyone'],\n",
    "    ['goodbye', 'world'],\n",
    "    ['goodbye', 'everyone']\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Accessing the word vector for 'hello'\n",
    "vector_hello = model.wv['hello']\n",
    "\n",
    "print(vector_hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, you can also find similar words, perform vector arithmetic to discover semantic relationships, and more.\n",
    "\n",
    "Remember, real-world embeddings are usually trained on vast corpora with billions of words, and as such, the embeddings capture richer semantic meanings than the small example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Advanced Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step guide to generating word embeddings using Word2Vec and a simple use case of leveraging those embeddings.\n",
    "\n",
    "### 1. Setup\n",
    "\n",
    "We'll begin byimporting the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the Data\n",
    "\n",
    "For this example, let's use some sample sentences. In a real-world scenario, you'd likely have a much larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['dog', 'barks'],\n",
    "    ['cat', 'meows'],\n",
    "    ['bird', 'sings'],\n",
    "    ['fish', 'swims'],\n",
    "    ['horse', 'gallops'],\n",
    "    ['koala', 'climbs'],\n",
    "    ['dolphin', 'jumps']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize the Embeddings\n",
    "\n",
    "To visualize the embeddings in 2D space, we can use PCA (Principal Component Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.key_to_index)\n",
    "embeddings = model.wv[words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Simple Use Case: Finding Similar Words\n",
    "\n",
    "Let's find words that are similar to \"dog\" using our trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('horse', topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('swims', topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return the top 5 words that are most similar to \"dog\" based on the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the simple use case of finding words similar to \"dog\":\n",
    "- \"gallops\" is the most similar with a similarity score of approximately 0.258.\n",
    "- \"dolphin\" is the next most similar with a similarity score of approximately 0.142.\n",
    "- ... and so on.\n",
    "\n",
    "For instance, in the figure, you might notice some clustering or proximity between words that have related meanings or contexts in the training data. However, it's worth noting that the quality and interpretability of embeddings heavily depend on the amount and quality of training data. In this case, our dataset was very small, so the embeddings might not capture nuanced semantic relationships as effectively as they would with a larger, more diverse dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Transformer architecture, used by GPT, embeddings are used to represent words in a dense vector space, as we discussed. Beyond these word embeddings, the Transformer introduces additional embeddings to facilitate the self-attention mechanism: the Query (Q), Key (K), and Value (V) matrices.\n",
    "\n",
    "### Meaning of Q, K, and V:\n",
    "\n",
    "1. **Query (Q)**: Represents the current word for which we want to determine attention scores. It's like asking: \"For this word, how much should I attend to other words in the sequence?\"\n",
    "2. **Key (K)**: Represents all the other words in terms of determining their relevance to the current word (the one represented by the query). It's the counterpart to the query in the attention calculation.\n",
    "3. **Value (V)**: Contains the information from the input words that we actually want to sum up in the attention mechanism. Once we've determined how much to \"attend\" to each word (using Q and K), we use the V vectors to compute the weighted sum.\n",
    "\n",
    "### How are Q, K, and V derived?\n",
    "\n",
    "For each word in the input, Q, K, and V are derived by multiplying the word's embedding by three weight matrices, \\(W_Q\\), \\(W_K\\), and \\(W_V\\), respectively. These weight matrices are learned during training.\n",
    "\n",
    "$$\n",
    "Q = \\text{Embedding} \\times W_Q\n",
    "$$\n",
    "$$\n",
    "K = \\text{Embedding} \\times W_K\n",
    "$$\n",
    "$$\n",
    "V = \\text{Embedding} \\times W_V\n",
    "$$\n",
    "\n",
    "### Python Example:\n",
    "\n",
    "Let's consider a simple example with small embeddings and weight matrices to compute Q, K, and V for a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple word embedding for the word \"hello\"\n",
    "embedding_hello = np.array([[0.1, 0.3, 0.5]])\n",
    "\n",
    "# Define weight matrices for Q, K, and V\n",
    "W_Q = np.array([[0.2, 0.8, 0.4],\n",
    "                [0.4, 0.1, 0.6],\n",
    "                [0.5, 0.7, 0.3]])\n",
    "\n",
    "W_K = np.array([[0.5, 0.3, 0.6],\n",
    "                [0.7, 0.8, 0.1],\n",
    "                [0.4, 0.2, 0.9]])\n",
    "\n",
    "W_V = np.array([[0.1, 0.6, 0.4],\n",
    "                [0.8, 0.5, 0.3],\n",
    "                [0.3, 0.7, 0.2]])\n",
    "\n",
    "# Compute Q, K, and V for the word \"hello\"\n",
    "Q_hello = np.dot(embedding_hello, W_Q)\n",
    "K_hello = np.dot(embedding_hello, W_K)\n",
    "V_hello = np.dot(embedding_hello, W_V)\n",
    "\n",
    "Q_hello, K_hello, V_hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the computed matrices for the word \"hello\":\n",
    "\n",
    "- $Q$ (Query) for \"hello\": \\([0.39, 0.46, 0.37]\\)\n",
    "- $K$ (Key) for \"hello\": \\([0.46, 0.37, 0.54]\\)\n",
    "- $V$ (Value) for \"hello\": \\([0.4, 0.56, 0.23]\\)\n",
    "\n",
    "In this simple example, these matrices are the result of multiplying the embedding for \"hello\" by the respective weight matrices $W_Q$, $W_K$, and $W_V$. In a real Transformer model, these weight matrices would be learned during training to optimize the self-attention mechanism.\n",
    "\n",
    "Keep in mind that in practice, these embeddings and matrices would be much larger (often with dimensions in the hundreds or even thousands) and would be used in combination with embeddings from other words in the sequence to compute the self-attention output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and Attention: A Simplified Overview\n",
    "\n",
    "1. **Transformers**: Transformers are a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They are primarily used for sequence-to-sequence tasks like language translation and text summarization. The core idea behind transformers is the self-attention mechanism.\n",
    "\n",
    "2. **Attention**: At a high level, attention allows the model to focus on different parts of the input sequence when producing an output. The intuition is that not all parts of the input sequence are equally relevant for a given output. Attention provides a weighted sum of the input sequence based on their relevance.\n",
    "\n",
    "### Simple Illustrative Example\n",
    "\n",
    "Imagine we have the sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "Our goal is to understand which words in the sentence are most \"relevant\" or \"attended to\" when trying to understand the word \"sat\".\n",
    "\n",
    "Let's assign some made-up attention scores to each word with respect to the word \"sat\":\n",
    "\n",
    "```\n",
    "The: 0.1\n",
    "cat: 0.3\n",
    "sat: 0.1\n",
    "on: 0.2\n",
    "the: 0.1\n",
    "mat: 0.2\n",
    "```\n",
    "\n",
    "These attention scores add up to 1 and represent the \"relevance\" of each word in understanding the word \"sat\". For example, \"cat\" has a higher relevance (0.3) than \"The\" (0.1) for understanding \"sat\".\n",
    "\n",
    "Let's visualize these attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "attention_scores = [0.1, 0.3, 0.1, 0.2, 0.1, 0.2]\n",
    "\n",
    "plt.bar(sentence, attention_scores)\n",
    "plt.ylabel('Attention Scores')\n",
    "plt.title('Attention Scores for the word \"sat\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Attention Scores are Computed\n",
    "\n",
    "The attention mechanism in transformers is based on the concept of **queries**, **keys**, and **values**. Each word in our input sequence (like our sentence \"The cat sat on the mat.\") gets transformed into these three representations.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. **Embedding**: First, each word in the sentence is transformed into an embedding (a dense vector representation).\n",
    "2. **Compute Q, K, V**: The embeddings are then transformed into query (Q), key (K), and value (V) representations using separate weight matrices for each transformation.\n",
    "3. **Calculate Attention Scores**: For a particular word (like \"sat\"), its query representation (Q) is used to calculate scores with all the key representations (K) of the other words in the sentence. This is usually done using a dot product.\n",
    "4. **Softmax**: The raw attention scores from the previous step are then passed through a softmax function to make them sum up to 1.\n",
    "5. **Compute Output**: The softmaxed attention scores are used to take a weighted sum of the value (V) representations. This gives the output representation for the word \"sat\".\n",
    "\n",
    "For simplicity, let's take a toy example. Imagine each word's embedding is a 2-dimensional vector, and our Q, K, V transformations are just 2x2 matrices. Let's compute attention scores for the word \"sat\" in our sentence.\n",
    "\n",
    "I'll generate some random embeddings for our words and random transformation matrices for Q, K, and V. Then, we'll compute the attention scores for the word \"sat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Toy embeddings for our sentence\n",
    "embeddings = {\n",
    "    \"The\": np.random.rand(2),\n",
    "    \"cat\": np.random.rand(2),\n",
    "    \"sat\": np.random.rand(2),\n",
    "    \"on\": np.random.rand(2),\n",
    "    \"the\": np.random.rand(2),\n",
    "    \"mat\": np.random.rand(2)\n",
    "}\n",
    "\n",
    "# Toy transformation matrices for Q, K, and V\n",
    "W_q = np.random.rand(2, 2)\n",
    "W_k = np.random.rand(2, 2)\n",
    "W_v = np.random.rand(2, 2)\n",
    "\n",
    "# Compute Q, K, V for each word\n",
    "Q = {word: np.dot(vec, W_q) for word, vec in embeddings.items()}\n",
    "K = {word: np.dot(vec, W_k) for word, vec in embeddings.items()}\n",
    "V = {word: np.dot(vec, W_v) for word, vec in embeddings.items()}\n",
    "\n",
    "# Compute raw attention scores for \"sat\"\n",
    "attention_raw_scores = {word: np.dot(Q[\"sat\"], vec) for word, vec in K.items()}\n",
    "\n",
    "# Softmax the scores\n",
    "attention_scores = {word: np.exp(score) / sum(np.exp(list(attention_raw_scores.values())))\n",
    "                    for word, score in attention_raw_scores.items()}\n",
    "\n",
    "attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our toy example with random embeddings and transformation matrices, here are the computed attention scores for the word \"sat\" in the sentence:\n",
    "\n",
    "```\n",
    "'The': 0.1708\n",
    "'cat': 0.1705\n",
    "'sat': 0.1567\n",
    "'on': 0.1653\n",
    "'the': 0.1703\n",
    "'mat': 0.1663\n",
    "```\n",
    "\n",
    "These scores represent the \"relevance\" of each word in understanding the context of \"sat\". In this example, all words have roughly similar attention scores because our embeddings and transformation matrices were generated randomly. In a trained transformer model, the embeddings and transformation matrices are learned from data, and the attention scores would reflect meaningful relationships between words.\n",
    "\n",
    "The final output representation for the word \"sat\" would be a weighted combination of the value (V) representations of all words, using the above attention scores as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's compute the final output representation for the word \"sat\" using the computed attention scores and the value (V) representations.\n",
    "\n",
    "The output representation for a word is calculated as a weighted sum of the value (V) representations of all words in the sentence, where the weights are the attention scores.\n",
    "\n",
    "Mathematically, for our word \"sat\", the output representation \\( \\text{output}_{\\text{sat}} \\) is given by:\n",
    "\n",
    "$$\n",
    "\\text{output}_{\\text{sat}} = \\sum_{\\text{word} \\in \\text{sentence}} \\text{attention\\_score}_{\\text{word}} \\times V_{\\text{word}}\n",
    "$$\n",
    "\n",
    "Let's compute this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output representation for \"sat\"\n",
    "output_sat = sum(attention_scores[word] * V[word] for word in sentence)\n",
    "\n",
    "output_sat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output representation for the word \"sat\" based on our toy example is approximately:\n",
    "\n",
    "$$\n",
    "\\text{output}_{\\text{sat}} = [0.4097, 0.3074]\n",
    "$$\n",
    "\n",
    "This 2-dimensional vector represents the context-aware representation of the word \"sat\" after considering the entire sentence. In a real transformer model, this representation would be much higher-dimensional (often 768 dimensions or more) and would capture rich contextual information.\n",
    "\n",
    "This context-aware representation can then be used for various downstream tasks like classification, translation, or generation, depending on the specific use case of the transformer model.\n",
    "\n",
    "To recap:\n",
    "\n",
    "1. We transformed each word into **Query (Q)**, **Key (K)**, and **Value (V)** representations.\n",
    "2. We computed attention scores for the word \"sat\" using its Q representation and the K representations of all other words.\n",
    "3. We obtained the final output representation for \"sat\" by taking a weighted sum of all V representations using the attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example\n",
    "def compute_attention_scores(sentence, embeddings, W_q, W_k, W_v):\n",
    "    # Compute Q, K, V for each word\n",
    "    Q = {word: np.dot(embeddings[word], W_q) for word in sentence}\n",
    "    K = {word: np.dot(embeddings[word], W_k) for word in sentence}\n",
    "    V = {word: np.dot(embeddings[word], W_v) for word in sentence}\n",
    "\n",
    "    # Compute raw attention scores for \"sat\"\n",
    "    attention_raw_scores = {word: np.dot(Q[\"sat\"], K[word]) for word in sentence}\n",
    "\n",
    "    # Softmax the scores\n",
    "    attention_scores = {\n",
    "        word: np.exp(score) / sum(np.exp(list(attention_raw_scores.values())))\n",
    "        for word, score in attention_raw_scores.items()\n",
    "    }\n",
    "\n",
    "    # Compute the output representation for \"sat\"\n",
    "    output_sat = np.sum([attention_scores[word] * V[word] for word in sentence], axis=0)\n",
    "    \n",
    "    return attention_scores, output_sat\n",
    "\n",
    "# Manually defined 2-dimensional embeddings for the sentence \"The cat sat on the mat.\"\n",
    "embeddings = {\n",
    "    \"The\": np.array([0.5, 0.2]),\n",
    "    \"cat\": np.array([0.9, 0.3]),\n",
    "    \"sat\": np.array([0.1, 0.8]),\n",
    "    \"on\": np.array([0.4, 0.6]),\n",
    "    \"the\": np.array([0.2, 0.3]),\n",
    "    \"mat\": np.array([0.7, 0.9])\n",
    "}\n",
    "\n",
    "# Manually defined 2x2 transformation matrices for Q, K, and V\n",
    "W_q = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "W_k = np.array([[0.5, 0.6], [0.7, 0.8]])\n",
    "W_v = np.array([[0.9, 1.0], [1.1, 1.2]])\n",
    "\n",
    "attention_scores, output_sat = compute_attention_scores(sentence, embeddings, W_q, W_k, W_v)\n",
    "print(attention_scores)\n",
    "print(output_sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Python is a first-class citizen in the AI world. The AI world is great.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(t, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = ['It is hot in here.', 'This is cold.', 'What a great weather.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(nt, padding=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(nt, padding=True, truncation=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(nt, padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='tf',\n",
    "                max_length=4\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT on a High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the basics of how GPT (Generative Pre-trained Transformer) models work. We'll do this in a step-by-step fashion, starting from the fundamental building blocks.\n",
    "\n",
    "### 1. Transformers\n",
    "\n",
    "At the heart of GPT is the Transformer architecture, which was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. The key innovation of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sequence relative to each other.\n",
    "\n",
    "### 2. Self-Attention Mechanism\n",
    "\n",
    "The main idea behind the self-attention mechanism is to compute a weighted sum of all words in a sequence based on their relevance to the current word. This is done by calculating three vectors for each word:\n",
    "- **Query (Q)**: Represents the current word.\n",
    "- **Key (K)**: Represents all the other words.\n",
    "- **Value (V)**: The actual values we want to sum up.\n",
    "\n",
    "The weight for each word is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q \\cdot K^T}{\\sqrt{d}} \\right) \\cdot V\n",
    "$$\n",
    "\n",
    "where \\(d\\) is the dimension of the query/key vectors.\n",
    "\n",
    "### 3. Multi-Head Attention\n",
    "\n",
    "In practice, the Transformer uses not just one set of Q, K, V matrices, but several. This allows the model to capture different types of relationships in the data. Each set of Q, K, V matrices is referred to as a \"head\", and the Transformer uses multiple heads in what's called \"multi-head attention\".\n",
    "\n",
    "### 4. Positional Encoding\n",
    "\n",
    "Since the Transformer does not have any inherent notion of the order of words, it uses positional encodings to give the model information about the position of words in a sequence. These encodings are added to the word embeddings at the input layer.\n",
    "\n",
    "### 5. GPT Specifics\n",
    "\n",
    "- **Masked Self-Attention**: While the original Transformer model is bidirectional (it looks at past and future words), GPT only looks at the past. This is done by masking future words in the self-attention mechanism.\n",
    "- **Stacked Layers**: Like other deep learning models, GPT stacks multiple layers of these Transformer blocks to capture more complex patterns.\n",
    "- **Fine-tuning for Tasks**: GPT can be fine-tuned for specific tasks by adding a simple output layer and training on task-specific data.\n",
    "\n",
    "### Simple Python Implementation\n",
    "\n",
    "Now, let's code a **very simplified** version of the self-attention mechanism to give you an idea of how it works in practice. Note that this will be a highly simplified example and won't capture the full complexity of GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def simple_self_attention(Q, K, V):\n",
    "    # Calculate the attention scores\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(Q.shape[1])\n",
    "    \n",
    "    # Apply the softmax to get the attention weights\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Multiply weights by the value matrix to get the final output\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define three simple word embeddings for \"Hello\", \"world\", and \"!\"\n",
    "hello_emb = np.array([1, 0, 0])\n",
    "world_emb = np.array([0, 1, 0])\n",
    "exclamation_emb = np.array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we'll use the same embeddings as Q, K, and V\n",
    "Q = np.array([hello_emb])\n",
    "K = np.array([hello_emb, world_emb, exclamation_emb])\n",
    "V = np.array([hello_emb, world_emb, exclamation_emb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the self-attention output\n",
    "output = simple_self_attention(Q, K, V)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a weighted sum of the word embeddings based on their relevance to the word \"Hello\" in our simple setup. The output is a combination of the embeddings for \"Hello\", \"world\", and \"!\", with the largest weight on the \"Hello\" embedding (as expected, since the query was \"Hello\").\n",
    "\n",
    "This is a highly simplified demonstration, but it provides a basic idea of how self-attention computes a weighted sum of word embeddings based on their relevance to a given word.\n",
    "\n",
    "To fully understand GPT:\n",
    "\n",
    "1. Imagine stacking many such attention layers.\n",
    "2. Introduce the multi-head mechanism to capture various relationships simultaneously.\n",
    "3. Add positional encoding to embed sequence order information.\n",
    "4. Use a much larger vocabulary and embedding size.\n",
    "5. Consider the entire training process involving large datasets and powerful GPUs.\n",
    "\n",
    "However, with this basic understanding, you have a foundation for delving deeper into the intricacies of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Padding & Truncation\n",
    "\n",
    "Both \"padding\" and \"truncation\" are techniques used in the preprocessing of sequences (like sentences or paragraphs) before feeding them to models like transformers.\n",
    "\n",
    "### Padding:\n",
    "\n",
    "Imagine you have a tray of cupcake slots, but not all slots are filled with cupcakes. To make every slot look occupied, you might place empty cupcake liners in the empty slots. Similarly, in the world of sequence models:\n",
    "\n",
    "- **Padding** refers to adding \"dummy\" or \"filler\" values to a sequence to make it reach a certain desired length.\n",
    "\n",
    "For example, if our model expects sequences of length 5 and we have the sentence \"I love cats\", which has only 3 words, we might \"pad\" it with two extra \"dummy\" words to make its length 5: \"I love cats [PAD] [PAD]\".\n",
    "\n",
    "### Truncation:\n",
    "\n",
    "On the flip side, imagine you have too many cupcakes for the tray. You might have to remove a few to make them fit. Similarly, in sequence processing:\n",
    "\n",
    "- **Truncation** refers to shortening a sequence by removing some of its elements to ensure it doesn't exceed a certain length.\n",
    "\n",
    "For instance, if our model can handle sequences of a maximum length of 5 and we have the sentence \"I really really love cats\", we might \"truncate\" it to fit the model: \"I really really love\".\n",
    "\n",
    "### Why are these important?\n",
    "\n",
    "Transformers, and many other neural network architectures, often expect input sequences of a fixed size. However, in real-world data, sentences and paragraphs can vary widely in length. Padding and truncation help standardize the lengths of these sequences:\n",
    "\n",
    "1. **Padding** ensures shorter sequences can fit the expected input size.\n",
    "2. **Truncation** ensures longer sequences don't exceed the model's capacity.\n",
    "\n",
    "However, it's worth noting that truncation can lead to loss of information if not done carefully. Similarly, excessive padding can sometimes reduce the efficiency of the model's training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Bert Model\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is one of the most influential models in the field of Natural Language Processing (NLP). Let's unpack its name and relevance in simple terms:\n",
    "\n",
    "### BERT Explained:\n",
    "\n",
    "1. **Bidirectional**: Traditional language models used to read text either from left to right or right to left. But when we humans read to understand something, we consider the whole context, looking at words both before and after a given word. BERT does the same; it reads text both ways (bidirectionally) to understand the context better.\n",
    "\n",
    "2. **Encoder Representations**: BERT is designed to \"encode\" or convert text into a form (like vectors) that machines can understand. By \"representations,\" we mean the machine-friendly format that still captures the essence and meaning of the original text.\n",
    "\n",
    "3. **Transformers**: BERT is built on the transformer architecture, which we discussed earlier. It uses the self-attention mechanism to weigh the importance of different words in a sentence relative to a given word.\n",
    "\n",
    "### Relevance of BERT:\n",
    "\n",
    "1. **Pre-training on Large Data**: BERT is trained on massive amounts of text, like the entire Wikipedia. This gives it a broad understanding of language.\n",
    "\n",
    "2. **Fine-tuning on Specific Tasks**: Once pre-trained, BERT can be \"fine-tuned\" on a smaller, specific dataset for tasks like question-answering, sentiment analysis, or text classification.\n",
    "\n",
    "3. **State-of-the-Art Performance**: When introduced, BERT achieved state-of-the-art results on various NLP benchmarks, making it a go-to model for many NLP tasks.\n",
    "\n",
    "4. **Versatility**: BERT can be adapted to a wide range of NLP tasks with only minimal changes to the model. You essentially add a small layer on top for your specific task and fine-tune BERT on task-specific data.\n",
    "\n",
    "5. **Variants and Offshoots**: BERT's success led to the development of various versions and offshoots tailored for different needs, such as DistilBERT (a smaller, faster version) and RoBERTa (optimized with more data and training tweaks).\n",
    "\n",
    "In summary, BERT revolutionized NLP by providing a robust and versatile model that understands context better than previous models. Its introduction marked a significant shift in how researchers and practitioners approached NLP tasks, leading to the rapid development and adoption of transformer-based models in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://hilpisch.com/tpq_logo.png' width=\"35%\" align=\"right\">\n",
    "\n",
    "<br><br><a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">ai@tpq.io</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
